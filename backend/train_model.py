import joblib
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error
import pickle

# Veri setini alt konulara daha fazla aÄŸÄ±rlÄ±k verecek ÅŸekilde gÃ¼ncelle
def generate_training_data(n_samples=1000):
    data = {}
    
    # Her Ã¶rnek iÃ§in rastgele deÄŸerler Ã¼ret
    for i in range(n_samples):
        sample = {}
        onceki_not = np.random.normal(70, 15)  # Ã–nceki not ortalamasÄ± 70, standart sapma 15
        
        # Alt konularÄ± bilme durumu (0-5 arasÄ±)
        alt_konu_bilgisi = [np.random.choice([0, 2, 3, 4, 5]) for _ in range(9)]
        
        # EÄŸer Ã¶nceki not yÃ¼ksekse (>80) ve alt konularÄ± iyi biliyorsa
        # motivasyon ve Ã§alÄ±ÅŸma sÃ¼resinin etkisini minimize et
        yuksek_performans = onceki_not > 80 and np.mean(alt_konu_bilgisi) > 3
        
        # Ana konular (alt konularÄ±n ortalamasÄ±na gÃ¶re)
        ana_konular = [
            np.mean(alt_konu_bilgisi[0:3]),
            np.mean(alt_konu_bilgisi[3:6]),
            np.mean(alt_konu_bilgisi[6:9])
        ]
        
        # Veri setine ekle
        sample.update({
            "Konu 1": ana_konular[0],
            "Konu 2": ana_konular[1],
            "Konu 3": ana_konular[2],
            "Konu 1 Alt 1": alt_konu_bilgisi[0],
            "Konu 1 Alt 2": alt_konu_bilgisi[1],
            "Konu 1 Alt 3": alt_konu_bilgisi[2],
            "Konu 2 Alt 1": alt_konu_bilgisi[3],
            "Konu 2 Alt 2": alt_konu_bilgisi[4],
            "Konu 2 Alt 3": alt_konu_bilgisi[5],
            "Konu 3 Alt 1": alt_konu_bilgisi[6],
            "Konu 3 Alt 2": alt_konu_bilgisi[7],
            "Konu 3 Alt 3": alt_konu_bilgisi[8],
            "Ã–nceki Not": onceki_not,
        })
        
        # YÃ¼ksek performanslÄ± Ã¶ÄŸrenciler iÃ§in motivasyon ve Ã§alÄ±ÅŸma sÃ¼resinin etkisini azalt
        if yuksek_performans:
            sample.update({
                "Ã‡alÄ±ÅŸma SÃ¼resi (saat)": np.random.uniform(0, 2),  # Minimal etki
                "Motivasyon (1-10)": np.random.uniform(1, 3)  # Minimal etki
            })
        else:
            sample.update({
                "Ã‡alÄ±ÅŸma SÃ¼resi (saat)": np.random.uniform(0, 12),
                "Motivasyon (1-10)": np.random.uniform(1, 10)
            })
        
        # Hedef notu hesapla
        alt_konu_etkisi = np.mean(alt_konu_bilgisi) * 15  # Alt konular max 75 puan etkisi
        onceki_not_etkisi = min(onceki_not * 0.25, 25)  # Ã–nceki not max 25 puan etkisi
        
        # YÃ¼ksek performanslÄ± Ã¶ÄŸrenciler iÃ§in diÄŸer faktÃ¶rlerin etkisini minimize et
        if yuksek_performans:
            motivasyon_etkisi = 0
            calisma_etkisi = 0
        else:
            motivasyon_etkisi = sample["Motivasyon (1-10)"] * 0.2  # Max 2 puan etkisi
            calisma_etkisi = sample["Ã‡alÄ±ÅŸma SÃ¼resi (saat)"] * 0.1  # Max 1.2 puan etkisi
        
        hedef_not = alt_konu_etkisi + onceki_not_etkisi + motivasyon_etkisi + calisma_etkisi
        sample["Hedef Not"] = np.clip(hedef_not, 0, 100)
        
        # Her Ã¶rneÄŸi veri setine ekle
        for key, value in sample.items():
            if key not in data:
                data[key] = []
            data[key].append(value)
    
    return data

# Yeni veri setini oluÅŸtur
data = generate_training_data(1000)

# ğŸ“Œ Veriyi DataFrame'e Ã§evir
df = pd.DataFrame(data)

# Ã–zellikler (X) ve hedef deÄŸiÅŸkeni (y) ayÄ±r
X = df.drop(columns=["Hedef Not"])
y = df["Hedef Not"]

# Veriyi eÄŸitim ve test setine ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ğŸ“Œ GridSearch parametreleri
param_grid = {
    "n_estimators": [100, 200],  # Daha fazla aÄŸaÃ§
    "learning_rate": [0.01, 0.05],  # Daha dÃ¼ÅŸÃ¼k Ã¶ÄŸrenme oranÄ±
    "max_depth": [5, 7],  # Daha derin aÄŸaÃ§lar
}

# GridSearch ile en iyi modeli bul
grid_search = GridSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_grid=param_grid,
    scoring="neg_mean_absolute_error",
    cv=3,
    verbose=1,
    n_jobs=-1,
)

# Modeli eÄŸit
grid_search.fit(X_train, y_train)

# En iyi modeli seÃ§
best_model = grid_search.best_estimator_
print(f"En iyi parametreler: {grid_search.best_params_}")

# Modeli kaydet
model_path = "optimized_gbr_model.pkl"
joblib.dump(best_model, model_path)
print(f"âœ… Optimize edilmiÅŸ model kaydedildi: {model_path}")

# Test setinde tahmin yap
y_pred = best_model.predict(X_test)
y_pred = np.clip(y_pred, 0, 100)  # Tahmin deÄŸerlerini 0-100 aralÄ±ÄŸÄ±nda tut

# Performans metrikleri
mae = mean_absolute_error(y_test, y_pred)
print(f"ğŸ“‰ Ortalama Mutlak Hata (MAE): {mae:.2f}")

# Ã–zelliklerin Ã¶nem sÄ±ralamasÄ±
feature_importances = best_model.feature_importances_
features = X.columns
print("\nğŸ“Š Ã–zellik Ã–nemi SÄ±ralamasÄ±:")
for feature, importance in sorted(zip(features, feature_importances), key=lambda x: x[1], reverse=True):
    print(f"{feature}: {importance:.4f}")

# Modeli pickle ile kaydet
with open('model.pkl', 'wb') as f:
    pickle.dump(best_model, f)
