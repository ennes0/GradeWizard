import joblib
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error
import pickle

# Daha gerÃ§ekÃ§i ve geniÅŸ bir veri seti oluÅŸturalÄ±m
def generate_synthetic_data(n_samples=1000):
    data = {
        # Ana konular (0-2 arasÄ±)
        "Konu 1": np.random.choice([0, 1, 2], n_samples),
        "Konu 2": np.random.choice([0, 1, 2], n_samples),
        "Konu 3": np.random.choice([0, 1, 2], n_samples),
        
        # Alt konular (0-5 arasÄ±, daha yÃ¼ksek aÄŸÄ±rlÄ±k)
        "Konu 1 Alt 1": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 1 Alt 2": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 1 Alt 3": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 2 Alt 1": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 2 Alt 2": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 2 Alt 3": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 3 Alt 1": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 3 Alt 2": np.random.choice([0, 2, 3, 4, 5], n_samples),
        "Konu 3 Alt 3": np.random.choice([0, 2, 3, 4, 5], n_samples),
        
        # Ã‡alÄ±ÅŸma sÃ¼resi (1-12 saat arasÄ±, gerÃ§ekÃ§i daÄŸÄ±lÄ±m)
        "Ã‡alÄ±ÅŸma SÃ¼resi (saat)": np.random.choice(
            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
            n_samples,
            p=[0.1, 0.15, 0.2, 0.15, 0.1, 0.1, 0.05, 0.05, 0.03, 0.03, 0.02, 0.02]  # GerÃ§ekÃ§i daÄŸÄ±lÄ±m
        ),
        
        # Ã–nceki notlar (0-100 arasÄ±, normal daÄŸÄ±lÄ±m)
        "Ã–nceki Not": np.clip(np.random.normal(70, 15, n_samples), 0, 100).astype(int),
        
        # Motivasyon (1-10 arasÄ±, gerÃ§ekÃ§i daÄŸÄ±lÄ±m)
        "Motivasyon (1-10)": np.random.choice(
            range(1, 11),
            n_samples,
            p=[0.05, 0.05, 0.1, 0.1, 0.15, 0.15, 0.15, 0.1, 0.1, 0.05]  # GerÃ§ekÃ§i motivasyon daÄŸÄ±lÄ±mÄ±
        )
    }
    
    # Hedef notu hesapla (daha gerÃ§ekÃ§i bir formÃ¼l ile)
    df = pd.DataFrame(data)
    
    # Alt konu bilgisine daha fazla aÄŸÄ±rlÄ±k veren bir hesaplama
    alt_konu_ortalama = df[[col for col in df.columns if 'Alt' in col]].mean(axis=1) * 10  # 0-50 arasÄ±
    ana_konu_ortalama = df[['Konu 1', 'Konu 2', 'Konu 3']].mean(axis=1) * 5  # 0-10 arasÄ±
    motivasyon_etkisi = df['Motivasyon (1-10)'] * 0.5  # 0.5-5 arasÄ±
    calisma_etkisi = df['Ã‡alÄ±ÅŸma SÃ¼resi (saat)'] * 0.5  # 0.5-6 arasÄ±
    onceki_not_etkisi = df['Ã–nceki Not'] * 0.3  # 0-30 arasÄ±
    
    # Hedef notu hesapla ve 0-100 aralÄ±ÄŸÄ±nda tut
    data["Hedef Not"] = np.clip(
        alt_konu_ortalama +  # En yÃ¼ksek etki alt konulardan (0-50)
        ana_konu_ortalama +  # Ana konulardan (0-10)
        motivasyon_etkisi +  # Motivasyondan (0.5-5)
        calisma_etkisi +     # Ã‡alÄ±ÅŸma sÃ¼resinden (0.5-6)
        onceki_not_etkisi,   # Ã–nceki nottan (0-30)
        0, 100
    ).astype(int)
    
    return data

# GeniÅŸletilmiÅŸ veri setini oluÅŸtur
data = generate_synthetic_data(1000)  # 1000 Ã¶rnek

# ğŸ“Œ Veriyi DataFrame'e Ã§evir
df = pd.DataFrame(data)

# Ã–zellikler (X) ve hedef deÄŸiÅŸkeni (y) ayÄ±r
X = df.drop(columns=["Hedef Not"])
y = df["Hedef Not"]

# Veriyi eÄŸitim ve test setine ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ğŸ“Œ GridSearch parametreleri
param_grid = {
    "n_estimators": [100, 200],  # Daha fazla aÄŸaÃ§
    "learning_rate": [0.01, 0.05],  # Daha dÃ¼ÅŸÃ¼k Ã¶ÄŸrenme oranÄ±
    "max_depth": [5, 7],  # Daha derin aÄŸaÃ§lar
}

# GridSearch ile en iyi modeli bul
grid_search = GridSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_grid=param_grid,
    scoring="neg_mean_absolute_error",
    cv=3,
    verbose=1,
    n_jobs=-1,
)

# Modeli eÄŸit
grid_search.fit(X_train, y_train)

# En iyi modeli seÃ§
best_model = grid_search.best_estimator_
print(f"En iyi parametreler: {grid_search.best_params_}")

# Modeli kaydet
model_path = "optimized_gbr_model.pkl"
joblib.dump(best_model, model_path)
print(f"âœ… Optimize edilmiÅŸ model kaydedildi: {model_path}")

# Test setinde tahmin yap
y_pred = best_model.predict(X_test)
y_pred = np.clip(y_pred, 0, 100)  # Tahmin deÄŸerlerini 0-100 aralÄ±ÄŸÄ±nda tut

# Performans metrikleri
mae = mean_absolute_error(y_test, y_pred)
print(f"ğŸ“‰ Ortalama Mutlak Hata (MAE): {mae:.2f}")

# Ã–zelliklerin Ã¶nem sÄ±ralamasÄ±
feature_importances = best_model.feature_importances_
features = X.columns
print("\nğŸ“Š Ã–zellik Ã–nemi SÄ±ralamasÄ±:")
for feature, importance in sorted(zip(features, feature_importances), key=lambda x: x[1], reverse=True):
    print(f"{feature}: {importance:.4f}")

# Modeli pickle ile kaydet
with open('model.pkl', 'wb') as f:
    pickle.dump(best_model, f)
